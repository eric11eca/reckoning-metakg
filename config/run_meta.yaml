data_param:
  dataset: owa_proof_2_hop
  dataset_type: proofwriter
  input_format: lm
  load_order: norm

model_param:
  model_type: gpt2
  model_name_or_path: gpt2-large

inner_param:
  n_inner_iter: 4
  inner_opt: adam
  inner_mode: lora
  
train_param:
  multi_task: true
  train_batch_size: 4
  predict_batch_size: 1
  num_train_epochs: 10
  learning_rate: 3e-5
  gradient_accumulation_steps: 1
  callback_monitor: val_acc_label
  wandb_checkpoint: true

peft_param:
  prefix_dim: 128
  lora_r: 16

util_param:
  device_idx: 0
  wandb_name: lora-l2
  checkpoint: ./output/model.ckpt