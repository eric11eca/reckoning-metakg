# @package _global_

defaults:
  - override /default: default.yaml

model_type: llama
model_name_or_path: meta-llama/Llama-3.2-1B

# experiment param:
do_train: true
do_eval: false

multi_task: true
baseline: true

use_lora: true
freeze_partial: false
condition: false
peft: true
bf16: true
attn_implementation: flash_attention_2

wandb_name: llama_3_2_1B-meta-4k
wandb_project: meta-memory

# checkpoint param:
load_checkpoint: false
checkpoint: None
wandb_model: false
wandb_checkpoint: false

# train param:
train_batch_size: 1
predict_batch_size: 1
eval_batch_size: 1
num_train_epochs: 2
callback_monitor: val_accuracy

# inner param:
n_inner_iter: 4
dyna_lr: true
inner_verbose: false
inner_mode: all

inner_lr: 1e-4                    # the learning rate used for inner loop optimization
inner_grad_accumulate: true       # whether to accumulate gradients in the inner loop
inner_accumulate_steps: 1         # the number of steps to accumulate gradients in the inner loop

# data_param:
dataset: student_records               # student_records, loss_in_mid, babilong
dataset_type: student_records           # student_records, loss_in_mid, babilong
load_order: in
no_facts: false
random_facts: false
no_question: false
max_eval_data: 0

device_idx: 0