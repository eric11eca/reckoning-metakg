# @package _global_

# model_param:
model_type: gpt2
model_name_or_path: gpt2-large

# path param:
train_dir: data
predict_dir: data
output_dir: output

# inner_param:
inner_opt: adam
inner_lr: 1e-5
inner_grad_accumulate: false
inner_accumulate_steps: 4
  
# train_param:
learning_rate: 3e-5
warmup_proportion: 0.06
weight_decay: 0.05
adam_epsilon: 1e-8
max_grad_norm: 0.1
gradient_accumulation_steps: 1
callback_monitor: val_acc_label
seed: 42
patience: 10
  
# peft_param:
prefix_dim: 128
lora_alpha: 32
lora_r: 16

# wandb_param:
wandb_name: multi
wandb_checkpoint: true
wandb_entity: epfl_nlp_phd
wandb_project: meta-knowledge
wandb_api_key: 9edee5b624841e10c88fcf161d8dc54d4efbee29

# util_param:
num_workers: 4