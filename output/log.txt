10/11/2022 09:55:36 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', dataset='proofwrite_owa_natlang', model='t5-base', output_dir='output', do_train=True, do_predict=False, predict_checkpoint='best-model.pt', inner_bsz=16, inner_lr=5e-05, custom_tasks_splits=None, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=3e-05, warmup_proportion=0.01, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=0.1, gradient_accumulation_steps=1, num_train_epochs=30.0, warmup_steps=500, total_steps=100000, wait_step=10000000000, verbose=False, eval_period=150, prefix='', debug=False, seed=42)
10/11/2022 09:55:36 - INFO - __main__ - output
10/11/2022 09:55:36 - INFO - __main__ - Using 0 gpus
10/11/2022 09:55:47 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', dataset='proofwrite_owa_natlang', model_name_or_path='t5-base', output_dir='output', do_train=True, do_predict=False, predict_checkpoint='best-model.pt', inner_bsz=16, inner_lr=5e-05, custom_tasks_splits=None, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=3e-05, warmup_proportion=0.01, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=0.1, gradient_accumulation_steps=1, num_train_epochs=30.0, warmup_steps=500, total_steps=100000, wait_step=10000000000, verbose=False, eval_period=150, prefix='', debug=False, seed=42)
10/11/2022 09:55:47 - INFO - __main__ - output
10/11/2022 09:55:47 - INFO - __main__ - Using 0 gpus
10/11/2022 09:56:02 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', dataset='proofwrite_owa_natlang', model_name_or_path='t5-base', output_dir='output', do_train=True, do_predict=False, predict_checkpoint='best-model.pt', inner_bsz=16, inner_lr=5e-05, custom_tasks_splits=None, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=3e-05, warmup_proportion=0.01, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=0.1, gradient_accumulation_steps=1, num_train_epochs=30.0, warmup_steps=500, total_steps=100000, wait_step=10000000000, verbose=False, eval_period=150, prefix='', debug=False, seed=42)
10/11/2022 09:56:02 - INFO - __main__ - output
10/11/2022 09:56:02 - INFO - __main__ - Using 0 gpus
10/11/2022 09:56:34 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', dataset='proofwrite_owa_natlang', model_name_or_path='t5-base', output_dir='output', do_train=True, do_predict=False, predict_checkpoint='best-model.pt', inner_bsz=16, inner_lr=5e-05, custom_tasks_splits=None, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=3e-05, warmup_proportion=0.01, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=0.1, gradient_accumulation_steps=1, num_train_epochs=30.0, warmup_steps=500, total_steps=100000, wait_step=10000000000, verbose=False, eval_period=150, prefix='', debug=False, seed=42)
10/11/2022 09:56:34 - INFO - __main__ - output
10/11/2022 09:56:34 - INFO - __main__ - Using 0 gpus
10/11/2022 09:56:48 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', dataset='proofwrite_owa_natlang', model_name_or_path='t5-base', output_dir='output', do_train=True, do_predict=False, predict_checkpoint='best-model.pt', inner_bsz=16, inner_lr=5e-05, custom_tasks_splits=None, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=3e-05, warmup_proportion=0.01, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=0.1, gradient_accumulation_steps=1, num_train_epochs=30.0, warmup_steps=500, total_steps=100000, wait_step=10000000000, verbose=False, eval_period=150, prefix='', debug=False, seed=42)
10/11/2022 09:56:48 - INFO - __main__ - output
10/11/2022 09:56:48 - INFO - __main__ - Using 0 gpus
10/11/2022 09:57:03 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', dataset='proofwriter_owa_natlang', model_name_or_path='t5-base', output_dir='output', do_train=True, do_predict=False, predict_checkpoint='best-model.pt', inner_bsz=16, inner_lr=5e-05, custom_tasks_splits=None, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=3e-05, warmup_proportion=0.01, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=0.1, gradient_accumulation_steps=1, num_train_epochs=30.0, warmup_steps=500, total_steps=100000, wait_step=10000000000, verbose=False, eval_period=150, prefix='', debug=False, seed=42)
10/11/2022 09:57:03 - INFO - __main__ - output
10/11/2022 09:57:03 - INFO - __main__ - Using 0 gpus
10/11/2022 09:57:05 - INFO - __main__ - Loading pre-tokenized data from data/proofwriter_owa_natlang/train.pt
10/11/2022 09:57:09 - INFO - __main__ - Start tokenizing ... 240 instances
10/11/2022 09:57:33 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', dataset='proofwriter_owa_natlang', model_name_or_path='t5-base', output_dir='output', expand_dev=False, do_train=True, do_predict=False, predict_checkpoint='best-model.pt', inner_bsz=16, inner_lr=5e-05, custom_tasks_splits=None, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=3e-05, warmup_proportion=0.01, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=0.1, gradient_accumulation_steps=1, num_train_epochs=30.0, warmup_steps=500, total_steps=100000, wait_step=10000000000, verbose=False, eval_period=150, prefix='', debug=False, seed=42)
10/11/2022 09:57:33 - INFO - __main__ - output
10/11/2022 09:57:33 - INFO - __main__ - Using 0 gpus
10/11/2022 09:57:35 - INFO - __main__ - Loading pre-tokenized data from data/proofwriter_owa_natlang/train.pt
10/11/2022 09:57:39 - INFO - __main__ - Start tokenizing ... 240 instances
10/11/2022 09:57:40 - INFO - __main__ - Loaded 240 examples from dev data
10/11/2022 09:58:04 - INFO - __main__ - Namespace(train_dir='data', predict_dir='data', dataset='proofwriter_owa_natlang', model_name_or_path='t5-base', output_dir='output', expand_dev=False, do_train=True, do_predict=False, predict_checkpoint='best-model.pt', inner_bsz=16, inner_lr=5e-05, custom_tasks_splits=None, checkpoint=None, do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=64, num_beams=4, append_another_bos=False, train_batch_size=1, predict_batch_size=1, learning_rate=3e-05, warmup_proportion=0.01, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=0.1, gradient_accumulation_steps=1, num_train_epochs=30.0, warmup_steps=500, total_steps=100000, wait_step=10000000000, verbose=False, eval_period=150, prefix='', debug=False, seed=42)
10/11/2022 09:58:04 - INFO - __main__ - output
10/11/2022 09:58:04 - INFO - __main__ - Using 0 gpus
10/11/2022 09:58:07 - INFO - __main__ - Loading pre-tokenized data from data/proofwriter_owa_natlang/train.pt
10/11/2022 09:58:10 - INFO - __main__ - Loading pre-tokenized data from data/proofwriter_owa_natlang/dev.pt
10/11/2022 09:58:40 - INFO - __main__ - Starting training!
