{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zechen/.conda/envs/kgmaml/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "pad = tokenizer.eos_token\n",
    "pad_id = tokenizer.encode(pad)\n",
    "\n",
    "train_input_ids_batch = []\n",
    "train_attention_mask_batch = []\n",
    "train_token_type_ids_batch = []\n",
    "\n",
    "data = {\n",
    "    \"facts\": [],\n",
    "    \"questions\": ['What is the most frequent letter?', 's']\n",
    "}\n",
    "\n",
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "num_facts = 8\n",
    "max_length = (1024 - 7) // num_facts\n",
    "for i in range(num_facts):\n",
    "    sequence = random.choices(letters, k=max_length)\n",
    "    data[\"facts\"].append(\" \".join(sequence))\n",
    "\n",
    "for i, fact in enumerate(data[\"facts\"]):\n",
    "    in_txt = \"\"\n",
    "    out_txt = fact\n",
    "    \n",
    "    ids1 = tokenizer(in_txt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    ids2 = tokenizer(out_txt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    n_mask = max_length - ids1.size(1) - ids2.size(1)\n",
    "    assert n_mask >= 0, (max_length, ids1.size(1), ids2.size(1))\n",
    "    padding = torch.LongTensor(pad_id * n_mask).unsqueeze(0)\n",
    "\n",
    "    input_ids = torch.cat((ids1, ids2, padding), dim=1)\n",
    "    attention_mask = torch.LongTensor(\n",
    "        [1] * (ids1.size(1) + ids2.size(1)) + [0] * n_mask).unsqueeze(0)\n",
    "    token_type_ids = torch.LongTensor(\n",
    "        [0] * ids1.size(1) + [1] * ids2.size(1) + [0] * n_mask).unsqueeze(0)\n",
    "\n",
    "    assert input_ids.size(1) == attention_mask.size(\n",
    "        1) == token_type_ids.size(1) == max_length\n",
    "    \n",
    "    train_input_ids_batch.append(input_ids)\n",
    "    train_attention_mask_batch.append(attention_mask)\n",
    "    train_token_type_ids_batch.append(token_type_ids)\n",
    "\n",
    "train_input_ids = torch.cat(train_input_ids_batch, dim=0)\n",
    "train_attention_mask = torch.cat(train_attention_mask_batch, dim=0)\n",
    "train_token_type_ids = torch.cat(train_token_type_ids_batch, dim=0)\n",
    "\n",
    "features = {\n",
    "    \"input_ids\": train_input_ids,\n",
    "    \"attention_mask\": train_attention_mask,\n",
    "    \"token_type_ids\": train_token_type_ids,\n",
    "    \"labels\": train_input_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the most frequent letter?\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "dev_input_ids_batch = []\n",
    "dev_attention_mask_batch = []\n",
    "dev_token_type_ids_batch = []\n",
    "\n",
    "qa = data[\"questions\"]\n",
    "in_txt = qa[0]\n",
    "print(in_txt)\n",
    "out_txt = qa[1]\n",
    "print(out_txt)\n",
    "\n",
    "ids1 = tokenizer(in_txt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "ids2 = tokenizer(out_txt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "n_mask = max_length - ids1.size(1) - ids2.size(1)\n",
    "assert n_mask >= 0, (max_length, ids1.size(1), ids2.size(1))\n",
    "padding = torch.LongTensor(pad_id * n_mask).unsqueeze(0)\n",
    "\n",
    "input_ids = torch.cat((ids1, ids2, padding), dim=1)\n",
    "attention_mask = torch.LongTensor(\n",
    "    [1] * (ids1.size(1) + ids2.size(1)) + [0] * n_mask).unsqueeze(0)\n",
    "token_type_ids = torch.LongTensor(\n",
    "    [0] * ids1.size(1) + [1] * ids2.size(1) + [0] * n_mask).unsqueeze(0)\n",
    "\n",
    "assert input_ids.size(1) == attention_mask.size(\n",
    "    1) == token_type_ids.size(1) == max_length\n",
    "\n",
    "dev_input_ids_batch.append(input_ids)\n",
    "dev_attention_mask_batch.append(attention_mask)\n",
    "dev_token_type_ids_batch.append(token_type_ids)\n",
    "\n",
    "dev_input_ids = torch.cat(dev_input_ids_batch, dim=0)\n",
    "dev_attention_mask = torch.cat(dev_attention_mask_batch, dim=0)\n",
    "dev_token_type_ids = torch.cat(dev_token_type_ids_batch, dim=0)\n",
    "\n",
    "dev_features = {\n",
    "    \"input_ids\": dev_input_ids,\n",
    "    \"attention_mask\": dev_attention_mask,\n",
    "    \"token_type_ids\": dev_token_type_ids,\n",
    "    \"labels\": dev_input_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda:5\")\n",
    "# model = torch.compile(model)\n",
    "\n",
    "model_params = []\n",
    "\n",
    "for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        model_params.append({\"params\": param, \"lr\": 1e-5})\n",
    "\n",
    "inner_optimizer = torch.optim.Adam(model_params, lr=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=3e-5, momentum=0.9)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "optimizer = inner_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.train()\n",
    "context = \" \".join(data[\"facts\"])\n",
    "context += f\"\\n {data['questions'][0]}\"\n",
    "tokenized = tokenizer([context], return_tensors=\"pt\").to(\"cuda:5\")\n",
    "tokenized[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Runtim: 0.1264362144470215s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "elapsed_times = []\n",
    "for n in tqdm(range(100)):\n",
    "    start = time.time()\n",
    "\n",
    "    #for i in range(18):\n",
    "    output = model(**tokenized, labels=tokenized[\"input_ids\"], use_cache=True)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    elapsed = end - start\n",
    "    elapsed_times.append(elapsed)\n",
    "\n",
    "avg_elapsed = sum(elapsed_times) / len(elapsed_times)\n",
    "print(f\"Avg Runtim: {avg_elapsed}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Runtim: 0.1264362144470215s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg Runtim: {avg_elapsed}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:35<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Runtim: 1.5005029726028443s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import higher\n",
    "\n",
    "for k in features:\n",
    "    features[k] = features[k].to(\"cuda:5\").long()\n",
    "\n",
    "for k in dev_features:\n",
    "    dev_features[k] = dev_features[k].to(\"cuda:5\").long()\n",
    "\n",
    "elapsed_times = []\n",
    "for n in tqdm(range(100)):\n",
    "    with higher.innerloop_ctx(\n",
    "        model, optimizer,\n",
    "        copy_initial_weights=False,\n",
    "        track_higher_grads=False) as (fmodel, diffopt):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for j in range(1):\n",
    "            outputs = fmodel(**features, use_cache=True)\n",
    "            inner_loss = outputs['loss']\n",
    "            diffopt.step(inner_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #for _ in range(18):\n",
    "            outputs = fmodel(**dev_features, use_cache=True)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        elapsed = end - start\n",
    "        elapsed_times.append(elapsed)\n",
    "\n",
    "avg_elapsed = sum(elapsed_times) / len(elapsed_times)\n",
    "print(f\"Avg Runtim: {avg_elapsed}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Runtim: 1.5005029726028443s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg Runtim: {avg_elapsed}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from meta_kg.optimizer import LSLRSchedular\n",
    "\n",
    "num_iterations = 5\n",
    "train_loss = []\n",
    "train_loop = tqdm(range(num_iterations))\n",
    "\n",
    "schedular = LSLRSchedular(\n",
    "    inner_optimizer,\n",
    "    num_inner_iter=num_iterations,\n",
    "    init_lr=1e-5\n",
    ")\n",
    "schedular.initialization(model.named_parameters())\n",
    "\n",
    "\n",
    "def compute_loss(features, outputs):\n",
    "    logits = outputs[\"logits\"][..., :-1, :].contiguous()\n",
    "    labels = features[\"input_ids\"][..., 1:].contiguous()\n",
    "    label_mask = features[\"token_type_ids\"][..., 1:].contiguous()\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    losses = loss_fct(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        labels.view(-1)\n",
    "    )\n",
    "    losses = losses.view(logits.size(0), logits.size(1)) * label_mask\n",
    "    loss = torch.sum(losses, axis=1) / torch.sum(label_mask, axis=1)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "model.train()\n",
    "for i in train_loop:\n",
    "    outputs = model(**features)\n",
    "    loss = outputs['loss']\n",
    "    train_loss.append(loss.item())\n",
    "    train_loop.set_description(f\"train_loss: {loss.item():.4f}\")\n",
    "    loss.backward()\n",
    "    schedular.step(model.named_parameters(), i)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(**features)\n",
    "    logits = outputs[\"logits\"][..., :-1, :].contiguous()\n",
    "    label_mask = features[\"token_type_ids\"][..., 1:].contiguous()\n",
    "    norm_logits = torch.softmax(logits, dim=-1)\n",
    "norm_logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_prob(prob_locs, norm_logits, input_ids, k=10):\n",
    "    topk_tokens_seq = []\n",
    "    for loc in prob_locs:\n",
    "        token = tokenizer.decode(input_ids[loc])\n",
    "        topk_prob = torch.topk(norm_logits[loc], k=k)\n",
    "        topk_tokens = []\n",
    "        for idx in topk_prob.indices:\n",
    "            gen_token = tokenizer.decode(idx.unsqueeze(0))\n",
    "            prob = round(norm_logits[loc][idx].item() * 100, 3)\n",
    "            topk_tokens.append((token.strip(), gen_token.strip(), prob))\n",
    "        topk_tokens_seq.append(topk_tokens)\n",
    "    return topk_tokens_seq\n",
    "\n",
    "batch_topk_tokens = []\n",
    "for i in range(norm_logits.size(0)):\n",
    "    input_ids = features[\"input_ids\"][i][1:]\n",
    "    norm_logits_item = norm_logits[i]* label_mask[i].unsqueeze(-1)\n",
    "    prob_locs = torch.nonzero(norm_logits_item)[:, 0].unique()\n",
    "    topk_tokens = get_token_prob(prob_locs, norm_logits_item, input_ids)\n",
    "    batch_topk_tokens.append(topk_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import higher\n",
    "from tqdm import tqdm \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda:2\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "num_iterations = 5\n",
    "loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "model.train()\n",
    "train_loop = tqdm(range(num_iterations))\n",
    "for i in range(num_iterations):\n",
    "    with higher.innerloop_ctx(\n",
    "        model, optimizer,\n",
    "        copy_initial_weights=False,\n",
    "        track_higher_grads=True) as (fmodel, diffopt):\n",
    "\n",
    "        for j in range(1):\n",
    "            outputs = fmodel(**features)\n",
    "            inner_loss = outputs['loss']\n",
    "            # logits = outputs[\"logits\"][..., :-1, :].contiguous()\n",
    "            # labels = features[\"input_ids\"][..., 1:].contiguous()\n",
    "            # label_mask = features[\"token_type_ids\"][..., 1:].contiguous()\n",
    "\n",
    "            # inner_losses = loss_fct(\n",
    "            #     logits.view(-1, logits.size(-1)),\n",
    "            #     labels.view(-1)\n",
    "            # )\n",
    "            # inner_losses = inner_losses.view(logits.size(0), logits.size(1)) * label_mask\n",
    "            # inner_loss = torch.sum(inner_losses, axis=1) / torch.sum(label_mask, axis=1)\n",
    "            # inner_loss = torch.mean(inner_loss)\n",
    "            diffopt.step(inner_loss)\n",
    "            print(f\"inner_loss: {inner_loss.item():.4f}\")\n",
    "\n",
    "        outputs = fmodel(**dev_features)\n",
    "        logits = outputs[\"logits\"][..., :-1, :].contiguous()\n",
    "        labels = dev_features[\"input_ids\"][..., 1:].contiguous()\n",
    "        label_mask = dev_features[\"token_type_ids\"][..., 1:].contiguous()\n",
    "\n",
    "        losses = loss_fct(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        losses = losses.view(logits.size(0), logits.size(1)) * label_mask\n",
    "        loss = torch.sum(losses, axis=1) / torch.sum(label_mask, axis=1)\n",
    "        loss = torch.mean(loss)\n",
    "        print(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgmaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e209e1d1adb2e949cd386c59b4af539fd4d52175cf17b5c0ba1825a24f2b66c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
