{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"distilgpt2\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"today is a beautiful day\"\n",
    "feature = tokenizer(text, return_tensors=\"pt\").to(\"mps\")\n",
    "outputs = model(**feature, labels=feature[\"input_ids\"])\n",
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_0: \n",
      "fact_1: \n",
      "fact_2: \n",
      "fact_3: \n",
      "fact_4: \n",
      "fact_5: \n",
      "fact_6: \n",
      "fact_7: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "max_length = 20\n",
    "pad = tokenizer.eos_token\n",
    "pad_id = tokenizer.encode(pad)\n",
    "\n",
    "train_input_ids_batch = []\n",
    "train_attention_mask_batch = []\n",
    "train_token_type_ids_batch = []\n",
    "\n",
    "# data = {\n",
    "#     \"facts\": ['person8 is a son to person11', 'person4 is a grandfather of person8'],\n",
    "#     \"questions\": ['How is person11 related to person4?', 'person4 is the father of person11']\n",
    "# }\n",
    "\n",
    "data = {\n",
    "    \"facts\": [\n",
    "        'orange is a fruit', \n",
    "        'apple is red',\n",
    "        'banana is yellow',\n",
    "        'apple is a fruit',\n",
    "        'banana is a fruit',\n",
    "        'orange is not a tree',\n",
    "        'apple is not spicy',\n",
    "        'banana is not spicy',],\n",
    "    \"questions\": ['orange is fruit?', 'yes']\n",
    "}\n",
    "\n",
    "# data[\"facts\"] = [\n",
    "#     \"apple is red\",\n",
    "#     \"banana is yellow\"\n",
    "# ]\n",
    "\n",
    "for i, fact in enumerate(data[\"facts\"]):\n",
    "    in_txt = f\"fact_{i}: \"\n",
    "    print(in_txt)\n",
    "    out_txt = fact\n",
    "    \n",
    "    ids1 = tokenizer(in_txt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    ids2 = tokenizer(out_txt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    n_mask = max_length - ids1.size(1) - ids2.size(1)\n",
    "    assert n_mask >= 0, (max_length, ids1.size(1), ids2.size(1))\n",
    "    padding = torch.LongTensor(pad_id * n_mask).unsqueeze(0)\n",
    "\n",
    "    input_ids = torch.cat((ids1, ids2, padding), dim=1)\n",
    "    attention_mask = torch.LongTensor(\n",
    "        [1] * (ids1.size(1) + ids2.size(1)) + [0] * n_mask).unsqueeze(0)\n",
    "    token_type_ids = torch.LongTensor(\n",
    "        [0] * ids1.size(1) + [1] * ids2.size(1) + [0] * n_mask).unsqueeze(0)\n",
    "\n",
    "    assert input_ids.size(1) == attention_mask.size(\n",
    "        1) == token_type_ids.size(1) == max_length\n",
    "    \n",
    "    train_input_ids_batch.append(input_ids)\n",
    "    train_attention_mask_batch.append(attention_mask)\n",
    "    train_token_type_ids_batch.append(token_type_ids)\n",
    "\n",
    "train_input_ids = torch.cat(train_input_ids_batch, dim=0)\n",
    "train_attention_mask = torch.cat(train_attention_mask_batch, dim=0)\n",
    "train_token_type_ids = torch.cat(train_token_type_ids_batch, dim=0)\n",
    "\n",
    "features = {\n",
    "    \"input_ids\": train_input_ids,\n",
    "    \"attention_mask\": train_attention_mask,\n",
    "    \"token_type_ids\": train_token_type_ids,\n",
    "    \"labels\": train_input_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orange is fruit?\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "dev_input_ids_batch = []\n",
    "dev_attention_mask_batch = []\n",
    "dev_token_type_ids_batch = []\n",
    "\n",
    "qa = data[\"questions\"]\n",
    "in_txt = qa[0]\n",
    "print(in_txt)\n",
    "out_txt = qa[1]\n",
    "print(out_txt)\n",
    "\n",
    "ids1 = tokenizer(in_txt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "ids2 = tokenizer(out_txt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "n_mask = max_length - ids1.size(1) - ids2.size(1)\n",
    "assert n_mask >= 0, (max_length, ids1.size(1), ids2.size(1))\n",
    "padding = torch.LongTensor(pad_id * n_mask).unsqueeze(0)\n",
    "\n",
    "input_ids = torch.cat((ids1, ids2, padding), dim=1)\n",
    "attention_mask = torch.LongTensor(\n",
    "    [1] * (ids1.size(1) + ids2.size(1)) + [0] * n_mask).unsqueeze(0)\n",
    "token_type_ids = torch.LongTensor(\n",
    "    [0] * ids1.size(1) + [1] * ids2.size(1) + [0] * n_mask).unsqueeze(0)\n",
    "\n",
    "assert input_ids.size(1) == attention_mask.size(\n",
    "    1) == token_type_ids.size(1) == max_length\n",
    "\n",
    "dev_input_ids_batch.append(input_ids)\n",
    "dev_attention_mask_batch.append(attention_mask)\n",
    "dev_token_type_ids_batch.append(token_type_ids)\n",
    "\n",
    "dev_input_ids = torch.cat(dev_input_ids_batch, dim=0)\n",
    "dev_attention_mask = torch.cat(dev_attention_mask_batch, dim=0)\n",
    "dev_token_type_ids = torch.cat(dev_token_type_ids_batch, dim=0)\n",
    "\n",
    "dev_features = {\n",
    "    \"input_ids\": dev_input_ids,\n",
    "    \"attention_mask\": dev_attention_mask,\n",
    "    \"token_type_ids\": dev_token_type_ids,\n",
    "    \"labels\": dev_input_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[22584,    62,    15,    25,   220, 43745,   318,   257,  8234, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [22584,    62,    16,    25,   220, 18040,   318,  2266, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [22584,    62,    17,    25,   220,  3820,  2271,   318,  7872, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [22584,    62,    18,    25,   220, 18040,   318,   257,  8234, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'evaluate': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def batch_split(row):\n",
    "    inner_loader = DataLoader(\n",
    "        row, batch_size=4, shuffle=False)\n",
    "    splited = [inner_batch for inner_batch in inner_loader]\n",
    "    return splited\n",
    "\n",
    "def batch_aggregate(rb):\n",
    "    inputs, masks, types = rb[0], rb[1], rb[2]\n",
    "    train_feature = {\n",
    "        \"input_ids\": inputs,\n",
    "        \"attention_mask\": masks,\n",
    "        \"token_type_ids\": types,\n",
    "        \"evaluate\": False\n",
    "    }\n",
    "    return train_feature\n",
    "\n",
    "batch = features\n",
    "rebatch = [batch_split(batch[key]) for key in batch]\n",
    "inner_train_features = [batch_aggregate(rb) for rb in zip(*rebatch)]\n",
    "\n",
    "inner_train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda:3\")\n",
    "\n",
    "model_params = []\n",
    "\n",
    "for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        model_params.append({\"params\": param, \"lr\": 1e-5})\n",
    "\n",
    "inner_optimizer = torch.optim.SGD(model_params, lr=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=3e-5, momentum=0.9)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "optimizer = inner_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss: 6.7593: 100%|██████████| 5/5 [00:00<00:00, 20.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from meta_kg.optimizer import LSLRSchedular\n",
    "\n",
    "num_iterations = 5\n",
    "train_loss = []\n",
    "train_loop = tqdm(range(num_iterations))\n",
    "\n",
    "schedular = LSLRSchedular(\n",
    "    inner_optimizer,\n",
    "    num_inner_iter=num_iterations,\n",
    "    init_lr=1e-5\n",
    ")\n",
    "schedular.initialization(model.named_parameters())\n",
    "\n",
    "\n",
    "def compute_loss(features, outputs):\n",
    "    logits = outputs[\"logits\"][..., :-1, :].contiguous()\n",
    "    labels = features[\"input_ids\"][..., 1:].contiguous()\n",
    "    label_mask = features[\"token_type_ids\"][..., 1:].contiguous()\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    losses = loss_fct(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        labels.view(-1)\n",
    "    )\n",
    "    losses = losses.view(logits.size(0), logits.size(1)) * label_mask\n",
    "    loss = torch.sum(losses, axis=1) / torch.sum(label_mask, axis=1)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "model.train()\n",
    "for i in train_loop:\n",
    "    outputs = model(**features)\n",
    "    loss = outputs['loss']\n",
    "    train_loss.append(loss.item())\n",
    "    train_loop.set_description(f\"train_loss: {loss.item():.4f}\")\n",
    "    loss.backward()\n",
    "    schedular.step(model.named_parameters(), i)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 19, 50257])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(**features)\n",
    "    logits = outputs[\"logits\"][..., :-1, :].contiguous()\n",
    "    label_mask = features[\"token_type_ids\"][..., 1:].contiguous()\n",
    "    norm_logits = torch.softmax(logits, dim=-1)\n",
    "norm_logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "person\n"
     ]
    }
   ],
   "source": [
    "def get_token_prob(prob_locs, norm_logits, input_ids, k=10):\n",
    "    topk_tokens_seq = []\n",
    "    for loc in prob_locs:\n",
    "        token = tokenizer.decode(input_ids[loc])\n",
    "        topk_prob = torch.topk(norm_logits[loc], k=k)\n",
    "        topk_tokens = []\n",
    "        for idx in topk_prob.indices:\n",
    "            gen_token = tokenizer.decode(idx.unsqueeze(0))\n",
    "            prob = round(norm_logits[loc][idx].item() * 100, 3)\n",
    "            topk_tokens.append((token.strip(), gen_token.strip(), prob))\n",
    "        topk_tokens_seq.append(topk_tokens)\n",
    "    return topk_tokens_seq\n",
    "\n",
    "batch_topk_tokens = []\n",
    "for i in range(norm_logits.size(0)):\n",
    "    input_ids = features[\"input_ids\"][i][1:]\n",
    "    norm_logits_item = norm_logits[i]* label_mask[i].unsqueeze(-1)\n",
    "    prob_locs = torch.nonzero(norm_logits_item)[:, 0].unique()\n",
    "    topk_tokens = get_token_prob(prob_locs, norm_logits_item, input_ids)\n",
    "    batch_topk_tokens.append(topk_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_loss: 6.3024\n",
      "loss: 10.7067\n",
      "inner_loss: 6.2737\n",
      "loss: 11.3563\n",
      "inner_loss: 6.4292\n",
      "loss: 10.5923\n",
      "inner_loss: 6.2354\n",
      "loss: 10.4774\n",
      "inner_loss: 6.2527\n",
      "loss: 10.6698\n"
     ]
    }
   ],
   "source": [
    "import higher\n",
    "from tqdm import tqdm \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda:2\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "num_iterations = 5\n",
    "loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "model.train()\n",
    "train_loop = tqdm(range(num_iterations))\n",
    "for i in range(num_iterations):\n",
    "    with higher.innerloop_ctx(\n",
    "        model, optimizer,\n",
    "        copy_initial_weights=False,\n",
    "        track_higher_grads=True) as (fmodel, diffopt):\n",
    "\n",
    "        for j in range(1):\n",
    "            outputs = fmodel(**features)\n",
    "            inner_loss = outputs['loss']\n",
    "            # logits = outputs[\"logits\"][..., :-1, :].contiguous()\n",
    "            # labels = features[\"input_ids\"][..., 1:].contiguous()\n",
    "            # label_mask = features[\"token_type_ids\"][..., 1:].contiguous()\n",
    "\n",
    "            # inner_losses = loss_fct(\n",
    "            #     logits.view(-1, logits.size(-1)),\n",
    "            #     labels.view(-1)\n",
    "            # )\n",
    "            # inner_losses = inner_losses.view(logits.size(0), logits.size(1)) * label_mask\n",
    "            # inner_loss = torch.sum(inner_losses, axis=1) / torch.sum(label_mask, axis=1)\n",
    "            # inner_loss = torch.mean(inner_loss)\n",
    "            diffopt.step(inner_loss)\n",
    "            print(f\"inner_loss: {inner_loss.item():.4f}\")\n",
    "\n",
    "        outputs = fmodel(**dev_features)\n",
    "        logits = outputs[\"logits\"][..., :-1, :].contiguous()\n",
    "        labels = dev_features[\"input_ids\"][..., 1:].contiguous()\n",
    "        label_mask = dev_features[\"token_type_ids\"][..., 1:].contiguous()\n",
    "\n",
    "        losses = loss_fct(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        losses = losses.view(logits.size(0), logits.size(1)) * label_mask\n",
    "        loss = torch.sum(losses, axis=1) / torch.sum(label_mask, axis=1)\n",
    "        loss = torch.mean(loss)\n",
    "        print(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "470f44e489c3417f40e6fbafbb8f6893ee0c258fcccc4737e0ea9152abfd2d49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
