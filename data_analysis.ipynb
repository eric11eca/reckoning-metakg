{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import random \n",
    "import copy\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from meta_kg.utils.py_io import *\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the|fail|or|naf)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(text))))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of facts: 7.33879781420765\n",
      "Average number of KG facts: 2.8278688524590163\n",
      "Average number of distractors: 4.5109289617486334\n",
      "Percent of distractors: 0.6184578703021291\n"
     ]
    }
   ],
   "source": [
    "proof_2_hop_d6 = read_jsonl('./data/owa_proof_2_hop_d5/dev.jsonl')\n",
    "num_facts = sum([len(p['all_facts']) for p in proof_2_hop_d6]) / len(proof_2_hop_d6)\n",
    "num_kg = sum([len(p['facts']) for p in proof_2_hop_d6]) / len(proof_2_hop_d6)\n",
    "num_distractors = sum([len(p['all_facts']) - len(p['facts'])\n",
    "                      for p in proof_2_hop_d6]) / len(proof_2_hop_d6)\n",
    "percent = sum([1 - len(p['facts']) / len(p['all_facts'])\n",
    "              for p in proof_2_hop_d6]) / len(proof_2_hop_d6)\n",
    "print(f'Average number of facts: {num_facts}')\n",
    "print(f'Average number of KG facts: {num_kg}')\n",
    "print(f'Average number of distractors: {num_distractors}')\n",
    "print(f'Percent of distractors: {percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of facts: 6.81056466302368\n",
      "Average number of KG facts: 2.814207650273224\n",
      "Average number of distractors: 3.9963570127504555\n",
      "Percent of distractors: 0.6004665846195858\n"
     ]
    }
   ],
   "source": [
    "proof_2_hop_d4 = read_jsonl('./data/owa_proof_2_hop_d4/dev.jsonl')\n",
    "num_facts = sum([len(p['all_facts']) for p in proof_2_hop_d4]) / len(proof_2_hop_d4)\n",
    "num_kg = sum([len(p['facts']) for p in proof_2_hop_d4]) / len(proof_2_hop_d4)\n",
    "num_distractors = sum([len(p['all_facts']) - len(p['facts'])\n",
    "                      for p in proof_2_hop_d4]) / len(proof_2_hop_d4)\n",
    "percent = sum([1 - len(p['facts']) / len(p['all_facts'])\n",
    "              for p in proof_2_hop_d4]) / len(proof_2_hop_d4)\n",
    "print(f'Average number of facts: {num_facts}')\n",
    "print(f'Average number of KG facts: {num_kg}')\n",
    "print(f'Average number of distractors: {num_distractors}')\n",
    "print(f'Percent of distractors: {percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_d4 = []\n",
    "for data in proof_2_hop_d4:\n",
    "    if data[\"answer\"] == \"unknown\":\n",
    "        none_d4.append(data)\n",
    "\n",
    "none_d6 = []\n",
    "for data in proof_2_hop_d6:\n",
    "    if data[\"answer\"] == \"unknown\":\n",
    "        none_d6.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'If something needs the bear and it needs the dog then the bear likes the dog.',\n",
       " 'The cat visits the dog.',\n",
       " 'The dog is nice.',\n",
       " 'The dog is round.'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(none_d6[0][\"all_facts\"]) - set(none_d4[0][\"all_facts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'If something likes the bear then the bear needs the cat.',\n",
       " 'The cat is nice.',\n",
       " 'The dog likes the bear.',\n",
       " 'The dog visits the cat.'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(none_d6[1][\"all_facts\"]) - set(none_d4[1][\"all_facts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If something likes the bear then it is rough.',\n",
       " 'If something is rough then it likes the dog.']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "none_d6[1][\"facts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.037945698397121363\n",
      "Accuracy (Relation):  0.6159633627739614\n",
      "Accuracy (KG):  0.04645076872751063\n",
      "F1 Score:  0.5012330757067278\n"
     ]
    }
   ],
   "source": [
    "test_out = read_json(\"./5_hop_d6.json\")\n",
    "answers = [data['answer'] for data in test_out]\n",
    "gen_outs = [data['gen_out'].split(\"?\")[1] for data in test_out]\n",
    "\n",
    "acc = 0\n",
    "acc_rel = 0\n",
    "acc_kg = 0\n",
    "f1_score = 0\n",
    "errors = []\n",
    "gen_rels = []\n",
    "relations = []\n",
    "for pred, truth in zip(gen_outs, answers):\n",
    "    acc += int(normalize_text(pred) == normalize_text(truth))\n",
    "    f1_score += compute_f1(pred, truth)\n",
    "    if normalize_text(pred) != normalize_text(truth):\n",
    "        errors.append((pred, truth))\n",
    "    relation = truth.split(\"because\")[0]\n",
    "    relations.append(relation)\n",
    "    facts = truth.split(\"because\")[1]\n",
    "    gen_rel = pred.split(\"because\")[0].strip()\n",
    "    gen_rels.append(gen_rel)\n",
    "    gen_facts = pred.split(\"because\")[1] if len(pred.split(\"because\")) > 1 else \"\"\n",
    "    acc_rel += int(normalize_text(gen_rel) == normalize_text(relation))\n",
    "    acc_kg += int(normalize_text(gen_facts) == normalize_text(facts))\n",
    "\n",
    "print(\"Accuracy: \", acc/len(gen_outs))\n",
    "print(\"Accuracy (Relation): \", acc_rel/len(gen_outs))\n",
    "print(\"Accuracy (KG): \", acc_kg/len(gen_outs))\n",
    "print(\"F1 Score: \", f1_score/len(gen_outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Accuracy yes:  0.9117647058823529\n",
      "Label Accuracy no:  0.8715686274509804\n",
      "Label Accuracy none:  0.07156862745098039\n",
      "Label Accuracy:  0.6182531894013739\n"
     ]
    }
   ],
   "source": [
    "test_out = read_json(\"./5_hop_d6.json\")\n",
    "answers = [data['answer'] for data in test_out]\n",
    "gen_outs = [data['gen_out'].split(\"?\")[1] for data in test_out]\n",
    "\n",
    "def clean_gen_label(gen):\n",
    "    if \"because\" in gen:\n",
    "        gen = gen.split(\"because\")[0].strip()\n",
    "    else:\n",
    "        gen = gen.split()[0].strip()\n",
    "    \n",
    "    if \",\" in gen:\n",
    "        gen = gen.split(\",\")[0].strip()\n",
    "    return gen\n",
    "\n",
    "labels = [t.split('because')[0].strip() for t in answers]\n",
    "gen_labels = [clean_gen_label(pred) for pred in gen_outs]\n",
    "em_label = [int(normalize_text(label) == normalize_text(gen))\n",
    "            for label, gen in zip(labels, gen_labels)]\n",
    "\n",
    "sorted = {}\n",
    "for label, gen in zip(labels, gen_labels):\n",
    "    if label not in sorted:\n",
    "        sorted[label] = [gen]\n",
    "    sorted[label].append(gen)\n",
    "for label, gens in sorted.items():\n",
    "    em = [int(normalize_text(label) == normalize_text(gen)) for gen in gens]\n",
    "    print(f\"Label Accuracy {label}: \", sum(em)/len(em))\n",
    "\n",
    "print(\"Label Accuracy: \", sum(em_label)/len(em_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (Label):  0.9836065573770492\n",
      "Exact Match (KG):  0.9781420765027322\n"
     ]
    }
   ],
   "source": [
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(truth) == normalize_text(prediction))\n",
    "\n",
    "targets = [data['answer'] for data in test_out]\n",
    "preds = [data['gen_out'].split(\"?\")[1] for data in test_out]\n",
    "\n",
    "if \"because\" in targets[0]:\n",
    "    labels = [t.split('because')[0].strip() for t in targets]\n",
    "    gen_labels = [p.split('because')[0].strip() for p in preds]\n",
    "    em_label = [compute_exact_match(\n",
    "        gen, label) for label, gen in zip(labels, gen_labels)]\n",
    "\n",
    "    facts = [t.split('because')[1].strip() for t in targets]\n",
    "    gen_kgs = [p.split('because')[1].strip() for p in preds]\n",
    "    em_kg = [compute_exact_match(\n",
    "        gen, label) for label, gen in zip(facts, gen_kgs)]\n",
    "    \n",
    "    print(\"Exact Match (Label): \", sum(em_label)/len(em_label))\n",
    "    print(\"Exact Match (KG): \", sum(em_kg)/len(em_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(errors[0][0], errors[0][1])\n",
    "compute_f1(errors[0][0], errors[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proof_3_hop= read_jsonl(\"./data/proof_3_hop/test.jsonl\")\n",
    "proof_3_hop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folio_train = read_jsonl(\"./data/folio/train.jsonl\")\n",
    "\n",
    "folio_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(data['facts']) for data in folio_train]\n",
    "set(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folio_data = []\n",
    "for data in folio_train:\n",
    "    folio_data.append({\n",
    "        \"guid\": str(uuid.uuid4()),\n",
    "        \"question\": data['conclusion'],\n",
    "        \"answer\": data['label'].lower(),\n",
    "        \"facts\": data['premises'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(folio_data, \"./data/folio/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "strategy = read_json(\"./data/strategyqa/strategyqa_train.json\")\n",
    "\n",
    "def parse_strategy(data):\n",
    "    question = data[\"question\"]\n",
    "    answer = \"yes\" if data[\"answer\"] else \"no\"\n",
    "    facts = [normalize_text(fact) for fact in data[\"facts\"]]\n",
    "    decomposition = data[\"decomposition\"]\n",
    "    example = {\n",
    "        \"guid\": str(uuid.uuid4()),\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"facts\": facts,\n",
    "        \"decomposition\": decomposition\n",
    "    }\n",
    "    return example\n",
    "\n",
    "strategy_data = [parse_strategy(data) for data in strategy]\n",
    "true_data = [data for data in strategy_data if data[\"answer\"] == \"yes\"]\n",
    "false_data = [data for data in strategy_data if data[\"answer\"] == \"no\"]\n",
    "\n",
    "train_true_data, dev_true_data = train_test_split(true_data, test_size=0.2, random_state=3042)\n",
    "train_false_data, dev_false_data = train_test_split(false_data, test_size=0.2, random_state=3042)\n",
    "\n",
    "train_data = train_true_data + train_false_data\n",
    "dev_data = dev_true_data + dev_false_data\n",
    "\n",
    "write_jsonl(train_data, \"./data/strategyqa/train.jsonl\")\n",
    "write_jsonl(dev_data, \"./data/strategyqa/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = read_jsonl(\"./data/taxonomy/hypernyms_training_mix_short_train.jsonl\")\n",
    "\n",
    "taxonomy_data = []\n",
    "for data in taxonomy:\n",
    "    data[\"guid\"] = data['id']\n",
    "    data[\"question\"] = normalize_text(data[\"phrase\"])\n",
    "    data[\"answer\"] = [\"no\", \"yes\"][data[\"answer\"]]\n",
    "    data[\"facts\"] = [normalize_text(fact) for fact in data[\"metadata\"][\"rules\"]]\n",
    "    example = {\n",
    "        \"guid\": data[\"guid\"],\n",
    "        \"question\": data[\"question\"],\n",
    "        \"answer\": data[\"answer\"],\n",
    "        \"facts\": data[\"facts\"],\n",
    "    }\n",
    "    taxonomy_data.append(example)\n",
    "taxonomy_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(taxonomy_data, \"./data/taxonomy/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting = read_jsonl(\"./data/counting/counting_training_mix_train.jsonl\")\n",
    "counting_data = []\n",
    "for data in counting:\n",
    "    data[\"guid\"] = data['id']\n",
    "    data[\"question\"] = data[\"phrase\"]\n",
    "    data[\"answer\"] = [\"no\", \"yes\"][data[\"answer\"]]\n",
    "    data[\"facts\"] = data[\"metadata\"][\"rules\"]\n",
    "    example = {\n",
    "        \"guid\": data[\"guid\"],\n",
    "        \"question\": data[\"question\"],\n",
    "        \"answer\": data[\"answer\"],\n",
    "        \"facts\": data[\"facts\"],\n",
    "    }\n",
    "    counting_data.append(example)\n",
    "counting_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(counting_data, \"./data/counting/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clutrr2 = read_jsonl(\"./data/clutrr_2_hop/train.jsonl\")\n",
    "clutrr4 = read_jsonl(\"./data/clutrr_4_hop/train.jsonl\")\n",
    "clutrr6 = read_jsonl(\"./data/clutrr_6_hop/train.jsonl\")\n",
    "print(len(clutrr2))\n",
    "print(len(clutrr4))\n",
    "print(len(clutrr6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clutrr_all = random.sample(clutrr2, 50000) + random.sample(clutrr4, 50000) + random.sample(clutrr6, 50000)\n",
    "len(clutrr_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(clutrr_all, \"./data/clutrr_mix/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musique = read_jsonl(\"./data/musique/musique_full_v1.0_dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsat = read_json(\"./data/arlsat/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entailment_tree(instance, add_distractors=False):\n",
    "    hop = instance['depth_of_proof']\n",
    "    hypothesis = instance['hypothesis']\n",
    "    triples = instance[\"meta\"][\"triples\"]\n",
    "    distractor_ids = instance[\"meta\"][\"distractors\"]\n",
    "    fact_id = list(set(triples.keys()) - set(distractor_ids))\n",
    "    distractors = [triples[idx] for idx in distractor_ids]\n",
    "    facts = [triples[idx] for idx in fact_id]\n",
    "\n",
    "    num_distractors = len(facts) // 2\n",
    "    to_add = random.choices(distractors, k=num_distractors)\n",
    "    if add_distractors:\n",
    "        facts.extend(to_add)\n",
    "    random.shuffle(facts)\n",
    "\n",
    "    for i, fact in enumerate(facts):\n",
    "        if random.randint(0, 1):\n",
    "            facts[i] = random.choice(distractors)\n",
    "    \n",
    "    valid_example = {\n",
    "        \"guid\": str(uuid.uuid4()),\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"facts\": facts,\n",
    "        \"answer\": \"yes\",\n",
    "    }\n",
    "\n",
    "    invalid_example = {\n",
    "        \"guid\": str(uuid.uuid4()),\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"facts\": facts,\n",
    "        \"answer\": \"no\",\n",
    "    }\n",
    "    return valid_example, invalid_example\n",
    "\n",
    "entail_tree = read_jsonl(\"./data/entailment_tree/task_2/test.jsonl\")\n",
    "\n",
    "entail_data = []\n",
    "for instance in entail_tree:\n",
    "    valid, invalid = parse_entailment_tree(instance, add_distractors=False)\n",
    "    entail_data.append(valid)\n",
    "    entail_data.append(invalid)\n",
    "len(entail_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(entail_data, \"./data/entailment_tree/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail_data = read_jsonl(\"./data/entailment_tree/train.jsonl\")\n",
    "depths = [len(instance['facts']) for instance in entail_data]\n",
    "max(depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "\n",
    "predictor = Predictor.from_path(\n",
    "    \"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_proofwrite_cwa(instance):\n",
    "    triples = {}\n",
    "    for k,v in instance[\"triples\"].items():\n",
    "        triples[k] = v[\"text\"]\n",
    "    rules = {}\n",
    "    for k,v in instance[\"rules\"].items():\n",
    "        rules[k] = v[\"text\"]\n",
    "    questions = []\n",
    "    for q in instance['questions'].values():\n",
    "        question = q['question']\n",
    "        answer = q['answer']\n",
    "        proofs = q['proofs']\n",
    "        if '@' not in proofs:\n",
    "            proofs = set(normalize_text(proofs).split())\n",
    "        else:\n",
    "            proofs = proofs.split('=')[1]\n",
    "            proofs = set(normalize_text(proofs).split())\n",
    "        if len(proofs) > 1:\n",
    "            questions.append((question, str(answer).lower(), proofs))\n",
    "    return triples, rules, questions\n",
    "\n",
    "def _add_prefix(text):\n",
    "    pool = [\"It's wrong to say\", \"It's false to say\", \"It's incorrect to say\", \"It's not true that\", \"It's not correct that\", \"It's not the case that\"]\n",
    "    return random.choice(pool) + \" \" + text.lower()\n",
    "\n",
    "def _add_suffix(text):\n",
    "    pool = [\"is not true\", \"is not correct\", \"is not the case\", \"is wrong\", \"is false\", \"is incorrect\"]\n",
    "    return text + \" \" + random.choice(pool)\n",
    "\n",
    "def adversarial(text, label):\n",
    "    if label == \"true\":\n",
    "        try:\n",
    "            verb = predictor.predict(sentence=text)['verbs'][0]['verb']\n",
    "        except:\n",
    "            verb = \"12345\"\n",
    "        if verb == \"is\" or verb == \"are\":\n",
    "            convert = text.replace(verb, verb + \" not\")\n",
    "        else:\n",
    "            convert = text.replace(verb, \"does not \" + verb)\n",
    "        \n",
    "        if random.uniform(0, 1) > 0.5:\n",
    "            return _add_prefix(convert)\n",
    "        else:\n",
    "            return _add_suffix(convert)\n",
    "    else:\n",
    "        convert = text.replace(\"not \", \"\")\n",
    "        if random.uniform(0, 1) > 0.5:\n",
    "            return _add_prefix(convert)\n",
    "        else:\n",
    "            return _add_suffix(convert)\n",
    "\n",
    "def build_example(triples, rules, question):\n",
    "    example = {}\n",
    "    triples.update(rules)\n",
    "    example['guid'] = str(uuid.uuid4())\n",
    "    example['answer'] = question[1].lower()\n",
    "    example['question'] = question[0]\n",
    "    example['proofs'] = list(question[2])\n",
    "    example['facts'] = [triples[k] for k in question[2]]\n",
    "    example['facts'] = list(set(example['facts']))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'triple1': 'Dave is cold.',\n",
       "  'triple2': 'Dave is smart.',\n",
       "  'triple3': 'Dave is white.',\n",
       "  'triple4': 'Dave is young.',\n",
       "  'triple5': 'Erin is cold.',\n",
       "  'triple6': 'Erin is kind.',\n",
       "  'triple7': 'Erin is red.',\n",
       "  'triple8': 'Erin is smart.',\n",
       "  'triple9': 'Erin is white.',\n",
       "  'triple10': 'Fiona is cold.',\n",
       "  'triple11': 'Fiona is kind.',\n",
       "  'triple12': 'Gary is kind.',\n",
       "  'triple13': 'Gary is red.',\n",
       "  'triple14': 'Gary is smart.'},\n",
       " {'rule1': 'If someone is white and red then they are nice.',\n",
       "  'rule2': 'All smart people are red.',\n",
       "  'rule3': 'If someone is kind and smart then they are cold.',\n",
       "  'rule4': 'Nice people are smart.',\n",
       "  'rule5': 'If someone is cold then they are white.',\n",
       "  'rule6': 'White people are young.',\n",
       "  'rule7': 'All white, young people are smart.',\n",
       "  'rule8': 'Nice people are white.',\n",
       "  'rule9': 'If Gary is kind and Gary is white then Gary is smart.'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hop = 5\n",
    "split = \"train\"\n",
    "owa = read_jsonl(f\"./data/proofwriter/OWA/depth-{hop}/meta-{split}.jsonl\")\n",
    "triples, rules, questions = parse_proofwrite_cwa(owa[0])\n",
    "triples, rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6330it [00:00, 6811.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "909it [00:00, 6735.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1794it [00:00, 6676.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4816it [00:01, 4116.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "719it [00:00, 4847.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1405it [00:00, 4632.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3322it [00:01, 2320.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "482it [00:00, 2523.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [00:00, 1908.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5175\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "for hop in [2,3,5]:\n",
    "    for split in [\"train\", \"dev\", \"test\"]:\n",
    "        owa = read_jsonl(f\"./data/proofwriter/OWA/depth-{hop}/meta-{split}.jsonl\")\n",
    "        owa_2_hop = []\n",
    "        for i, data in tqdm(enumerate(owa)):\n",
    "            triples, rules, questions = parse_proofwrite_cwa(data)\n",
    "            all_facts = copy.copy(list(triples.values()) + list(rules.values()))\n",
    "\n",
    "            kgs = list(data[\"triples\"].values()) + list(data[\"rules\"].values())\n",
    "            assert len(kgs) == len(all_facts)\n",
    "\n",
    "            examples = [build_example(triples, rules, q) for q in questions]\n",
    "            # all_facts = [fact for e in examples for fact in e['facts']]\n",
    "            # all_facts = list(set(all_facts))\n",
    "            # for example in examples:\n",
    "            #     example[\"all_facts\"] = all_facts\n",
    "            for example in examples:\n",
    "                # example[\"all_facts\"] = all_facts\n",
    "                pool = set(all_facts) - set(example['facts'])\n",
    "                if len(pool) > 1:\n",
    "                    example['all_facts'] = example['facts'] + (\n",
    "                        random.choices(list(pool), k=5))\n",
    "                    example['all_facts'] = list(set(example['all_facts']))\n",
    "                else:\n",
    "                    example['all_facts'] = example['facts']\n",
    "            true_data = [e for e in examples if e['answer'] == \"true\"]\n",
    "            false_data = [e for e in examples if e['answer'] == \"false\"]\n",
    "            unknown_data = [e for e in examples if e['answer'] == \"unknown\"]\n",
    "            owa_2_hop.extend(random.choices(true_data, k=len(unknown_data)))\n",
    "            owa_2_hop.extend(random.choices(false_data, k=len(unknown_data)))\n",
    "            owa_2_hop.extend(unknown_data)\n",
    "        \n",
    "        print(len(owa_2_hop))\n",
    "        os.makedirs(f\"./data/owa_proof_{hop}_hop_d5\", exist_ok=True)\n",
    "        write_jsonl(owa_2_hop, f\"./data/owa_proof_{hop}_hop_d5/{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Uploading directory ./data/owa_proof_2_hop_d5 to: \"epfl_nlp_phd/data-collection/owa_proof_2_hop_d5:latest\" (dataset)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/owa_proof_2_hop_d5)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchenze_epfl\u001b[0m (\u001b[33mepfl_nlp_phd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/zemingchen/Desktop/meta-knowledge/wandb/run-20230301_141630-3quz47b1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrestful-durian-103\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/epfl_nlp_phd/data-collection\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/epfl_nlp_phd/data-collection/runs/3quz47b1\u001b[0m\n",
      "Artifact uploaded, use this artifact in a run by adding:\n",
      "\n",
      "    artifact = run.use_artifact(\"epfl_nlp_phd/data-collection/owa_proof_2_hop_d5:latest\")\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrestful-durian-103\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/epfl_nlp_phd/data-collection/runs/3quz47b1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230301_141630-3quz47b1/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Uploading directory ./data/owa_proof_3_hop_d5 to: \"epfl_nlp_phd/data-collection/owa_proof_3_hop_d5:latest\" (dataset)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/owa_proof_3_hop_d5)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchenze_epfl\u001b[0m (\u001b[33mepfl_nlp_phd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/zemingchen/Desktop/meta-knowledge/wandb/run-20230301_141648-394c7270\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33measy-hill-104\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/epfl_nlp_phd/data-collection\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/epfl_nlp_phd/data-collection/runs/394c7270\u001b[0m\n",
      "Artifact uploaded, use this artifact in a run by adding:\n",
      "\n",
      "    artifact = run.use_artifact(\"epfl_nlp_phd/data-collection/owa_proof_3_hop_d5:latest\")\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33measy-hill-104\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/epfl_nlp_phd/data-collection/runs/394c7270\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230301_141648-394c7270/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Uploading directory ./data/owa_proof_5_hop_d5 to: \"epfl_nlp_phd/data-collection/owa_proof_5_hop_d5:latest\" (dataset)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/owa_proof_5_hop_d5)... Done. 0.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchenze_epfl\u001b[0m (\u001b[33mepfl_nlp_phd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/zemingchen/Desktop/meta-knowledge/wandb/run-20230301_141705-26mnwk5z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbreezy-sponge-105\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/epfl_nlp_phd/data-collection\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/epfl_nlp_phd/data-collection/runs/26mnwk5z\u001b[0m\n",
      "Artifact uploaded, use this artifact in a run by adding:\n",
      "\n",
      "    artifact = run.use_artifact(\"epfl_nlp_phd/data-collection/owa_proof_5_hop_d5:latest\")\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mbreezy-sponge-105\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/epfl_nlp_phd/data-collection/runs/26mnwk5z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230301_141705-26mnwk5z/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! ./upload_wandb_data.sh ./data/owa_proof_2_hop_d2 owa_proof_2_hop_d2\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_3_hop_d2 owa_proof_3_hop_d2\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_5_hop_d2 owa_proof_5_hop_d2\n",
    "\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_2_hop_d4 owa_proof_2_hop_d4\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_3_hop_d4 owa_proof_3_hop_d4\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_5_hop_d4 owa_proof_5_hop_d4\n",
    "\n",
    "! ./upload_wandb_data.sh ./data/owa_proof_2_hop_d5 owa_proof_2_hop_d5\n",
    "! ./upload_wandb_data.sh ./data/owa_proof_3_hop_d5 owa_proof_3_hop_d5\n",
    "! ./upload_wandb_data.sh ./data/owa_proof_5_hop_d5 owa_proof_5_hop_d5\n",
    "\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_2_hop_d6 owa_proof_2_hop_d6\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_3_hop_d6 owa_proof_3_hop_d6\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_5_hop_d6 owa_proof_5_hop_d6\n",
    "\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_2_hop_d10 owa_proof_2_hop_dall\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_3_hop_d10 owa_proof_3_hop_dall\n",
    "# ! ./upload_wandb_data.sh ./data/owa_proof_5_hop_d10 owa_proof_5_hop_dall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owa = read_jsonl(\"./data/proofwriter/OWA/NatLang/test.jsonl\")\n",
    "owa_add = read_jsonl(\"./data/proofwriter/OWA/birds-electricity/birds-electricity.jsonl\")\n",
    "owa_test = owa + owa_add\n",
    "\n",
    "answer2label = {\n",
    "    \"true\": \"entailment\",\n",
    "    \"false\": \"contradiction\",\n",
    "    \"unknown\": \"neutral\"\n",
    "}\n",
    "\n",
    "proofwriter_nli = []\n",
    "for data in owa_test:\n",
    "    triples, rules, questions = parse_proofwrite_cwa(data)\n",
    "    premise = list(triples.values()) + list(rules.values())\n",
    "    premise = \" \".join(premise)\n",
    "    for q in questions:\n",
    "        hypothesis = q[0]\n",
    "        label = q[1]\n",
    "        proofwriter_nli.append({\n",
    "            \"guid\": str(uuid.uuid4()),\n",
    "            \"premise\": premise,\n",
    "            \"hypothesis\": hypothesis,\n",
    "            \"label\": answer2label[label]\n",
    "        })\n",
    "\n",
    "proofwriter_nli[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(proofwriter_nli, \"./data/proofwriter_nli/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop = 10\n",
    "\n",
    "pth = f\"./data/clutrr-system/forward/test/{hop}/long_proof_1.{hop}_test_facts_ANON.txt\"\n",
    "\n",
    "def parse_clutrr_line(line):\n",
    "    data = line.replace('<STORY>', \"\")\n",
    "    data = data.replace('<QUERY>', \"<>\")\n",
    "    data = data.replace('<ANSWER>', \"<>\")\n",
    "    data = data.replace('<PROOF>', \"<>\")\n",
    "    data = data.replace('ent_', \"person \")\n",
    "    data = data.split(\"<>\")\n",
    "    data = [d.strip() for d in data]\n",
    "    facts = data[0]\n",
    "    question = data[1]\n",
    "    answer = data[-1]\n",
    "    return {\n",
    "        \"guid\": str(uuid.uuid4()),\n",
    "        \"question\": question,\n",
    "        \"facts\": facts,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "\n",
    "with open(pth, \"r\") as reader:\n",
    "    lines = reader.readlines()\n",
    "    clutrr = [parse_clutrr_line(line) for line in lines]\n",
    "write_jsonl(clutrr, f\"./data/clutrr-system/test_{hop}_hop.jsonl\")\n",
    "clutrr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clutrr = read_jsonl(\"data/clutrr/dev.jsonl\")\n",
    "\n",
    "clutrr_4 = [x for x in clutrr if len(x[\"facts\"]) == 4]\n",
    "clutrr_6 = [x for x in clutrr if len(x[\"facts\"]) == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = [\n",
    "    \"son\", \"daughter\",\n",
    "    \"brother\", \"sister\",\n",
    "    \"father\", \"mother\",\n",
    "    \"husband\", \"wife\",\n",
    "    \"grandfather\", \"grandmother\",\n",
    "    \"grandson\", \"granddaughter\",\n",
    "    \"uncle\", \"aunt\",\n",
    "    \"son-in-law\", \"daughter-in-law\",\n",
    "    \"father-in-law\", \"mother-in-law\",\n",
    "    \"brother-in-law\", \"sister-in-law\",\n",
    "    \"nephew\", \"niece\"\n",
    "]\n",
    "\n",
    "persons = [\n",
    "    'A', 'B', 'C', 'D', \n",
    "    'H', 'J', 'K', 'L', \n",
    "    'M', 'N', 'O', 'P', \n",
    "    'Q', 'R', 'S', 'T',\n",
    "    'V', 'X', 'Y', 'Z',]\n",
    "\n",
    "entity_map = {}\n",
    "for i, p in enumerate(persons):\n",
    "    entity_map[p] = str(i+1)\n",
    "\n",
    "def get_knowledge(tokens):\n",
    "    entity = []\n",
    "    relation = None\n",
    "    for tok in tokens:\n",
    "        if tok.isdigit():\n",
    "            entity.append(persons[int(tok)-1])\n",
    "        if tok in rels:\n",
    "            relation = tok\n",
    "    assert len(entity) == 2\n",
    "    if relation is None:\n",
    "        print(tokens)\n",
    "    return entity, relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(dataset):\n",
    "    simple_dataset = copy.deepcopy(dataset)\n",
    "    for data in simple_dataset:\n",
    "        facts = []\n",
    "        facts_raw = data['facts'].split(\". \")\n",
    "        for fact in facts_raw:\n",
    "            tokens = fact.split()\n",
    "            entity, relation = get_knowledge(tokens)\n",
    "            facts.append([' '.join(entity), relation])\n",
    "        data['facts'] = facts\n",
    "        question = data['question']\n",
    "        answer = data['answer']\n",
    "        tokens = answer.split()\n",
    "        entity, relation = get_knowledge(tokens)\n",
    "        qa_pair = []\n",
    "        \n",
    "        qa_pair.append(question)\n",
    "        # qa_pair[0] = qa_pair[0].replace(entity_map[entity[0]], entity[0])\n",
    "        # qa_pair[0] = qa_pair[0].replace(entity_map[entity[1]], entity[1])\n",
    "        # qa_pair[0] = qa_pair[0].replace(\"person \", \"\")\n",
    "        qa_pair[0] = f\"How are {entity[0]} and {entity[1]} related to each other ?\"\n",
    "        qa_pair.append(' '.join(entity))\n",
    "        qa_pair.append(relation)\n",
    "        data['questions'] = [qa_pair]\n",
    "    return simple_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hop = 10\n",
    "clutrr = read_jsonl(f\"data/clutrr-system/test_{hop}_hop.jsonl\")\n",
    "simple_clutrr =  simplify(clutrr)\n",
    "\n",
    "os.makedirs(f\"data/clutrr_{hop}_hop\", exist_ok=True)\n",
    "write_jsonl(simple_clutrr, f\"data/clutrr_{hop}_hop/test.jsonl\")\n",
    "\n",
    "simple_clutrr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_clutrr_4 = simplify(clutrr_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(simple_clutrr_4, \"data/clutrr_4_hop/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_clutrr_6 = simplify(clutrr_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(simple_clutrr_6, \"data/clutrr_6_hop/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_out_4_hop = read_json(\"./output/20221212-033351/dev_out-epoch=0_step=5061.json\")\n",
    "eval_out_4_hop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0 \n",
    "for data in eval_out_4_hop:\n",
    "    gen_out= data['gen_out'].split(\"?\")\n",
    "    gen_answer = gen_out[1].strip()\n",
    "    if gen_answer == data['answer']:\n",
    "        acc += 1\n",
    "print(acc/len(eval_out_4_hop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proof_5_hop = read_jsonl(\"./data/proof_5_hop_hard/train.jsonl\")\n",
    "\n",
    "sort_by_proof = {}\n",
    "for data in proof_5_hop:\n",
    "    key = \",\".join(data['facts'])\n",
    "    if key not in sort_by_proof:\n",
    "        sort_by_proof[key] = [data]\n",
    "    else:\n",
    "        sort_by_proof[key].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_k = [len(data['facts']) for data in proof_5_hop]\n",
    "max(num_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sort_by_proof), len(proof_5_hop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_question = list(sort_by_proof.items())\n",
    "multi_question[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "meta_2_hop = [0.988, 0.5113, 0.414, 0.3526, 0.316]\n",
    "meta_4_hop = [0.99, 0.9631, 0.819, 0.6452, 0.4608]\n",
    "meta_6_hop = [0.9994, 0.9592, 0.9731, 0.9208, 0.799]\n",
    "meta_clutrr = [meta_2_hop, meta_4_hop, meta_6_hop]\n",
    "\n",
    "cmap = sns.cm.rocket_r\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    meta_clutrr, \n",
    "    annot=True, \n",
    "    square=True, \n",
    "    linewidth=3.0, \n",
    "    xticklabels=[2, 4, 6, 8, 10],\n",
    "    yticklabels=[2, 4, 6],\n",
    "    cbar=False,\n",
    "    cmap=cmap)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_2_hop = [0.981, 0.4432, 0.3717, 0.3258, 0.217]\n",
    "baseline_4_hop = [0.9117, 0.8924, 0.8044, 0.7734, 0.6063]\n",
    "baseline_6_hop = [1.0, 0.9706, 0.9546, 0.9054, 0.7622]\n",
    "baseline_clutrr = [baseline_2_hop, baseline_4_hop, baseline_6_hop]\n",
    "\n",
    "cmap = sns.cm.rocket_r\n",
    "ax = sns.heatmap(\n",
    "    baseline_clutrr, \n",
    "    annot=True, \n",
    "    square=True, \n",
    "    linewidth=3.0, \n",
    "    xticklabels=[2, 4, 6, 8, 10],\n",
    "    yticklabels=[2, 4, 6], \n",
    "    cbar=False,\n",
    "    cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "proof_2_hop = [0.998, 0.961, 0.863]\n",
    "proof_3_hop = [0, 0.956, 0.866]\n",
    "proof_5_hop = [0, 0 , 0.977]\n",
    "\n",
    "proof = [proof_2_hop, proof_3_hop, proof_5_hop]\n",
    "\n",
    "mask = 1 - np.triu(np.ones_like(proof, dtype=np.bool))\n",
    "mask = [[0,0,0],\n",
    "        [1,0,0],\n",
    "        [1,1,0]]\n",
    "mask = np.array(mask)\n",
    "heatmap = sns.heatmap(proof, mask=mask, xticklabels=[2,3,5], yticklabels=[2,3,5], vmin=0, vmax=1, annot=True, cmap='Blues', cbar=False, annot_kws={\"fontsize\":18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use([\"nature\", \"grid\", \"ieee\"])\n",
    "\n",
    "# sns.set(font_scale=1.5)\n",
    "\n",
    "x = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "plt.figure(figsize=(5, 4))\n",
    "meta_6_hop = [100,95.5,96,94.65,94.85,95.7,90,84.5,80]\n",
    "baseline_6_hop = [100,95.4,90,89.1,90.34,87.6,81.6,75,67.9]\n",
    "\n",
    "plt.plot(\n",
    "    x, meta_6_hop, \n",
    "    'o-', color='#fdb462', \n",
    "    alpha=1.0, label='Meta-kg-6', \n",
    "    linewidth='2', ms=5)\n",
    "plt.plot(\n",
    "    x, baseline_6_hop, \n",
    "    's-', color='#7fb1d3', \n",
    "    alpha=1.0, label='Baseline-6', \n",
    "    linewidth='2', ms=5)\n",
    "\n",
    "plt.ylim(20, 100)\n",
    "\n",
    "#plt.grid(axis='x', color='0.95')\n",
    "plt.legend()\n",
    "plt.title('6-hop Clutrr Generalization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2, 4, 6, 8, 10]\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(x, meta_4_hop, 'o-', color='orange', alpha=0.9, label='Meta-kg')\n",
    "plt.plot(x, baseline_4_hop, 's-', color='blue', alpha=0.9, label='Baseline')\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "plt.grid(axis='x', color='0.95')\n",
    "plt.legend()\n",
    "plt.title('4-hop Clutrr Generalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_2_hop = [0.988, 0.5113, 0.414, 0.3526, 0.316]\n",
    "baseline_2_hop = [0.981, 0.4432, 0.3717, 0.3258, 0.217]\n",
    "x = [2, 4, 6, 8, 10]\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(x, meta_2_hop, 'o-', color='orange', alpha=0.9, label='Meta-kg')\n",
    "plt.plot(x, baseline_2_hop, 's-', color='blue', alpha=0.9, label='Baseline')\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "plt.grid(axis='x', color='0.95')\n",
    "plt.legend()\n",
    "plt.title('2-hop Clutrr Generalization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "470f44e489c3417f40e6fbafbb8f6893ee0c258fcccc4737e0ea9152abfd2d49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
