{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import random \n",
    "import copy\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from meta_kg.utils.py_io import *\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the|fail|or|naf)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(text))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "strategy = read_json(\"./data/strategyqa/strategyqa_train.json\")\n",
    "\n",
    "def parse_strategy(data):\n",
    "    question = data[\"question\"]\n",
    "    answer = \"yes\" if data[\"answer\"] else \"no\"\n",
    "    facts = [normalize_text(fact) for fact in data[\"facts\"]]\n",
    "    decomposition = data[\"decomposition\"]\n",
    "    example = {\n",
    "        \"guid\": str(uuid.uuid4()),\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"facts\": facts,\n",
    "        \"decomposition\": decomposition\n",
    "    }\n",
    "    return example\n",
    "\n",
    "strategy_data = [parse_strategy(data) for data in strategy]\n",
    "true_data = [data for data in strategy_data if data[\"answer\"] == \"yes\"]\n",
    "false_data = [data for data in strategy_data if data[\"answer\"] == \"no\"]\n",
    "\n",
    "train_true_data, dev_true_data = train_test_split(true_data, test_size=0.2, random_state=3042)\n",
    "train_false_data, dev_false_data = train_test_split(false_data, test_size=0.2, random_state=3042)\n",
    "\n",
    "train_data = train_true_data + train_false_data\n",
    "dev_data = dev_true_data + dev_false_data\n",
    "\n",
    "write_jsonl(train_data, \"./data/strategyqa/train.jsonl\")\n",
    "write_jsonl(dev_data, \"./data/strategyqa/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'afdf807efc8b55e8b1c9349c0ac554ca',\n",
       " 'question': 'reggae is capable of express feelings',\n",
       " 'answer': 'yes',\n",
       " 'facts': ['music is capable of express feelings',\n",
       "  'reggae is music',\n",
       "  'holly is capable of branch out',\n",
       "  'plant is not capable of express feelings',\n",
       "  'tulip is plant',\n",
       "  'reggae is auditory communication',\n",
       "  'mustard is not capable of shade from sun',\n",
       "  'reggae is not vertebrate']}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomy = read_jsonl(\"./data/taxonomy/hypernyms_training_mix_short_dev.jsonl\")\n",
    "\n",
    "taxonomy_data = []\n",
    "for data in taxonomy:\n",
    "    data[\"guid\"] = data['id']\n",
    "    data[\"question\"] = normalize_text(data[\"phrase\"])\n",
    "    data[\"answer\"] = [\"np\", \"yes\"][data[\"answer\"]]\n",
    "    data[\"facts\"] = [normalize_text(fact) for fact in data[\"metadata\"][\"rules\"]]\n",
    "    example = {\n",
    "        \"guid\": data[\"guid\"],\n",
    "        \"question\": data[\"question\"],\n",
    "        \"answer\": data[\"answer\"],\n",
    "        \"facts\": data[\"facts\"],\n",
    "    }\n",
    "    taxonomy_data.append(example)\n",
    "taxonomy_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(taxonomy_data, \"./data/taxonomy/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': '8f836112dccfa8f61fc934f114145a2c',\n",
       " 'question': 'James H. Roberts is the CEO of ADT.',\n",
       " 'answer': 'np',\n",
       " 'facts': ['Timothy J. Whall is the CEO of ADT.', 'ADT has one CEO.']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counting = read_jsonl(\"./data/counting/counting_training_mix_dev.jsonl\")\n",
    "counting_data = []\n",
    "for data in counting:\n",
    "    data[\"guid\"] = data['id']\n",
    "    data[\"question\"] = data[\"phrase\"]\n",
    "    data[\"answer\"] = [\"np\", \"yes\"][data[\"answer\"]]\n",
    "    data[\"facts\"] = data[\"metadata\"][\"rules\"]\n",
    "    example = {\n",
    "        \"guid\": data[\"guid\"],\n",
    "        \"question\": data[\"question\"],\n",
    "        \"answer\": data[\"answer\"],\n",
    "        \"facts\": data[\"facts\"],\n",
    "    }\n",
    "    counting_data.append(example)\n",
    "counting_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(counting_data, \"./data/counting/dev.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96011\n",
      "89971\n",
      "90922\n"
     ]
    }
   ],
   "source": [
    "clutrr2 = read_jsonl(\"./data/clutrr_2_hop/train.jsonl\")\n",
    "clutrr4 = read_jsonl(\"./data/clutrr_4_hop/train.jsonl\")\n",
    "clutrr6 = read_jsonl(\"./data/clutrr_6_hop/train.jsonl\")\n",
    "print(len(clutrr2))\n",
    "print(len(clutrr4))\n",
    "print(len(clutrr6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clutrr_all = random.sample(clutrr2, 50000) + random.sample(clutrr4, 50000) + random.sample(clutrr6, 50000)\n",
    "len(clutrr_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(clutrr_all, \"./data/clutrr_mix/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "musique = read_jsonl(\"./data/musique/musique_full_v1.0_dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsat = read_json(\"./data/arlsat/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_entailment_tree(instance, add_distractors=False):\n",
    "    hop = instance['depth_of_proof']\n",
    "    hypothesis = instance['hypothesis']\n",
    "    triples = instance[\"meta\"][\"triples\"]\n",
    "    distractor_ids = instance[\"meta\"][\"distractors\"]\n",
    "    fact_id = list(set(triples.keys()) - set(distractor_ids))\n",
    "    distractors = [triples[idx] for idx in distractor_ids]\n",
    "    facts = [triples[idx] for idx in fact_id]\n",
    "\n",
    "    num_distractors = len(facts) // 2\n",
    "    to_add = random.choices(distractors, k=num_distractors)\n",
    "    if add_distractors:\n",
    "        facts.extend(to_add)\n",
    "    random.shuffle(facts)\n",
    "\n",
    "    for i, fact in enumerate(facts):\n",
    "        if random.randint(0, 1):\n",
    "            facts[i] = random.choice(distractors)\n",
    "    \n",
    "    valid_example = {\n",
    "        \"guid\": str(uuid.uuid4()),\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"facts\": facts,\n",
    "        \"answer\": \"yes\",\n",
    "    }\n",
    "\n",
    "    invalid_example = {\n",
    "        \"guid\": str(uuid.uuid4()),\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"facts\": facts,\n",
    "        \"answer\": \"no\",\n",
    "    }\n",
    "    return valid_example, invalid_example\n",
    "\n",
    "entail_tree = read_jsonl(\"./data/entailment_tree/task_2/test.jsonl\")\n",
    "\n",
    "entail_data = []\n",
    "for instance in entail_tree:\n",
    "    valid, invalid = parse_entailment_tree(instance, add_distractors=False)\n",
    "    entail_data.append(valid)\n",
    "    entail_data.append(invalid)\n",
    "len(entail_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(entail_data, \"./data/entailment_tree/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entail_data = read_jsonl(\"./data/entailment_tree/train.jsonl\")\n",
    "depths = [len(instance['facts']) for instance in entail_data]\n",
    "max(depths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "owa = read_jsonl(\"./data/proofwriter/OWA/depth-2/meta-test.jsonl\")\n",
    "\n",
    "def parse_proofwrite_cwa(instance):\n",
    "    triples = {}\n",
    "    for k,v in instance[\"triples\"].items():\n",
    "        triples[k] = v[\"text\"]\n",
    "    rules = {}\n",
    "    for k,v in instance[\"rules\"].items():\n",
    "        rules[k] = v[\"text\"]\n",
    "    questions = []\n",
    "    for q in instance['questions'].values():\n",
    "        question = q['question']\n",
    "        answer = q['answer']\n",
    "        proofs = q['proofs']\n",
    "        if '@' not in proofs:\n",
    "            proofs = set(normalize_text(proofs).split())\n",
    "        else:\n",
    "            proofs = proofs.split('=')[1]\n",
    "            proofs = set(normalize_text(proofs).split())\n",
    "        if len(proofs) > 1:\n",
    "            questions.append((question, str(answer).lower(), proofs))\n",
    "    return triples, rules, questions\n",
    "\n",
    "def build_example(triples, rules, question):\n",
    "    example = {}\n",
    "    triples.update(rules)\n",
    "    example['guid'] = str(uuid.uuid4())\n",
    "    example['question'] = question[0]\n",
    "    example['answer'] = question[1]\n",
    "    example['proofs'] = list(question[2])\n",
    "    example['facts'] = [triples[k] for k in question[2]]\n",
    "    example['facts'] = list(set(example['facts']))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "owa_2_hop = []\n",
    "for i, data in enumerate(owa):\n",
    "    triples, rules, questions = parse_proofwrite_cwa(data)\n",
    "    examples = [build_example(triples, rules, q) for q in questions]\n",
    "    owa_2_hop.extend(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(owa_2_hop, \"./data/owa_proof_2_hop/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clutrr = read_jsonl(\"data/clutrr/dev.jsonl\")\n",
    "\n",
    "clutrr_4 = [x for x in clutrr if len(x[\"facts\"]) == 4]\n",
    "clutrr_6 = [x for x in clutrr if len(x[\"facts\"]) == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = [\n",
    "    \"son\", \"daughter\",\n",
    "    \"brother\", \"sister\",\n",
    "    \"father\", \"mother\",\n",
    "    \"husband\", \"wife\",\n",
    "    \"grandfather\", \"grandmother\",\n",
    "    \"grandson\", \"granddaughter\",\n",
    "    \"uncle\", \"aunt\",\n",
    "    \"son-in-law\", \"daughter-in-law\",\n",
    "    \"father-in-law\", \"mother-in-law\",\n",
    "    \"brother-in-law\", \"sister-in-law\",\n",
    "    \"nephew\", \"niece\"\n",
    "]\n",
    "\n",
    "persons = [\n",
    "    'A', 'B', 'C', 'D', \n",
    "    'H', 'J', 'K', 'L', \n",
    "    'M', 'N', 'O', 'P', \n",
    "    'Q', 'R', 'S', 'T',\n",
    "    'V', 'X', 'Y', 'Z',]\n",
    "\n",
    "def get_knowledge(tokens):\n",
    "    entitiy = []\n",
    "    relation = None\n",
    "    for tok in tokens:\n",
    "        if tok.isdigit():\n",
    "            entitiy.append(persons[int(tok)-1])\n",
    "        if tok in rels:\n",
    "            relation = tok\n",
    "    assert len(entitiy) == 2\n",
    "    if relation is None:\n",
    "        print(tokens)\n",
    "    return ' '.join(entitiy), relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(dataset):\n",
    "    simple_dataset = copy.deepcopy(dataset)\n",
    "    for data in simple_dataset:\n",
    "        facts = []\n",
    "        for fact in data['facts']:\n",
    "            tokens = fact.split()\n",
    "            entity, relation = get_knowledge(tokens)\n",
    "            facts.append((entity, relation))\n",
    "        data['facts'] = facts\n",
    "        for question in data['questions']:\n",
    "            answer = question[1]\n",
    "            tokens = answer.split()\n",
    "            entity, relation = get_knowledge(tokens)\n",
    "            question[0] = f\"How are {entity[0]} and {entity[-1]} related to each other ?\"\n",
    "            question[1] = entity\n",
    "            assert len(question[0].split()) == 10\n",
    "            question.append(relation)\n",
    "    return simple_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_clutrr_4 = simplify(clutrr_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(simple_clutrr_4, \"data/clutrr_4_hop/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_clutrr_6 = simplify(clutrr_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(simple_clutrr_6, \"data/clutrr_6_hop/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'e6e0679a-2081-4863-bbb2-13b08fe283e9',\n",
       " 'prefix': 'clutrr_4_hop',\n",
       " 'question': '<|endoftext|>C B aunt\\nH J sister\\nB J daughter\\nZ H daughter\\nBased on fact_0 fact_1 fact_2 fact_3, How are C and Z related to each other ?',\n",
       " 'gen_out': 'C B aunt\\nH J sister\\nB J daughter\\nZ H daughter\\nBased on fact_0 fact_1 fact_2 fact_3, How are C and Z related to each other?daughter',\n",
       " 'answer': 'daughter'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_out_4_hop = read_json(\"./output/20221212-033351/dev_out-epoch=0_step=5061.json\")\n",
    "eval_out_4_hop[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n"
     ]
    }
   ],
   "source": [
    "acc = 0 \n",
    "for data in eval_out_4_hop:\n",
    "    gen_out= data['gen_out'].split(\"?\")\n",
    "    gen_answer = gen_out[1].strip()\n",
    "    if gen_answer == data['answer']:\n",
    "        acc += 1\n",
    "print(acc/len(eval_out_4_hop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "470f44e489c3417f40e6fbafbb8f6893ee0c258fcccc4737e0ea9152abfd2d49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
